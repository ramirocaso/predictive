---
title: "Cluster Jerárquico"
author: "Ramiro Casó"
date: "8/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Intro

En este tutorial haremos clustering jerárquico, usando como base el mismo data set de Ford Ka, Antes, carguemos los paquetes que necesitamos. 


```{r message=FALSE, warning=FALSE, paged.print=FALSE}

library(readxl)     # Lectura o carga de datos
library(tidyr)      # Pipe y manipulación de datos. 
library(tidyverse)  # Manipulación de datos. 
library(cluster)    # Clusters
library(factoextra) # Visualización 
library(dendextend) # Comparación de dengramas (esto es muy cool pero no lo he hecho aún)
```

Seguidamente cargamos el set de datos. Lo haremos utilizando el data set de Ford Ka. Noten que cargué la hoja que ya tenía en mi working directory. En el comando de read_excel, específiqué la hoja que quería y, además, le quité las primeras 5 líneas, que no contienen información. 

```{r message=FALSE, warning=FALSE}
psicograficos <- read_excel("DataSets/Ford_Ka_Students.xls", 
    sheet = "Psychographic Data", skip = 5)
head(psicograficos)
```


### Normalización

Lo que sigue es normalizar los datos. Hay que dejar en la matriz de datos solo las columnas que usaremos para agrupar, de modo que hay que sacar la primera columna. 
```{r}
norm_psico <- scale(psicograficos[,-1])
```


### Clustering
Como vamos a hacer clustering jerárquico usaremos función *agnes* (su nombre viene de "agglomerative nesting").

Para poder hacerlo, tenemos que calcular la disimilaridades en la data, usando la función dist. 
```{r}
# Matriz de disimilaridad
d <- dist(norm_psico, method = "euclidean")

# Clustering jerárquico usando el método "complete" de distancia. 
hc1 <- hclust(d, method = "complete" )

# construcción del dendograma
plot(hc1, cex = 0.6, hang = -1)
```

Veamos que pasa si usamos otros métodos de cálculo de distancia entre clusters. En este caso, single. 
```{r}
# Matriz de disimilaridad es la misma que el caso anterior y está guardada en el objeto d

# Clustering jerárquico usando el método "single" de distancia. 
hc2 <- hclust(d, method = "single" )

# construcción del dendograma
plot(hc2, cex = 0.6, hang = -1)
```

Finalmente, probemos el método average. 
```{r}
# Matriz de disimilaridad es la misma que el caso anterior y está guardada en el objeto d

# Clustering jerárquico usando el método "single" de distancia. 
hc3 <- hclust(d, method = "average")

# construcción del dendograma
plot(hc3, cex = 0.6, hang = -1)
```

La altura de las fusiones en el dendograma, que se pueden apreciar en el eje verticual, indica la disimilaridad entre dos observaciones. Mientras más alta sea la fusión o unión, más diferentes son los observaciones. Nótese que la proximidad de las observaciones NO es un indicador similaridad. Solo la altura arroja esa información. 
En razón de lo anterior, la altura a la que se corta un dendograma nos el número de cluster que queremos, similar a la selección de K en k-means. Para poder identificar subgrupos, podemos cortar el dendograma con la función *cutree*

```{r}
# Usaremos el método "complete" que fue el que arrojó el dendograma más claro. 
hc5 <- hclust(d, method = "complete" )

# Cortaremos el dendograma en 4 clusters. 
sub_grp <- cutree(hc5, k = 4)

# Acá podemos ver los subgrupos creados. 
table(sub_grp)

```

Y si queremos agregar las columnas de membresías al data set original, podemos usar la función *mutate* de la siguiente forma. 
```{r}
psicograficos %>%
  mutate(cluster = sub_grp) %>%
  head
```

Se puede inclusive hacer un dendograma resaltando los 4 clusters, de la siguiente forma: 
```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 4, border = 2:5)
```

Finalmente, intentemos ver si podemos visualizar los clusters usando la función *fviz_cluster*

```{r}
fviz_cluster(list(data = psicograficos, cluster = sub_grp))
```

